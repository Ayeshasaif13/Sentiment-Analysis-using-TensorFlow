{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "994d6af3-4290-4ae5-b7a4-10c25a9f9943",
      "metadata": {
        "id": "994d6af3-4290-4ae5-b7a4-10c25a9f9943"
      },
      "source": [
        "# Task 1: Create a DataFrame from the File Containing Sentence-Label Pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38b1c426-3cfd-42a6-9da9-321a9d66b2c3",
      "metadata": {
        "id": "38b1c426-3cfd-42a6-9da9-321a9d66b2c3"
      },
      "source": [
        "## Task Overview  \n",
        "In this task, you'll construct a **Pandas DataFrame** from a text file containing sentences and their corresponding labels. Each line in the file follows this format:\n",
        "\n",
        "```\n",
        "sentence@label\n",
        "```\n",
        "\n",
        "Your goal is to:  \n",
        "- Read data from **`Sentences_75Agree_sample.txt`**.  \n",
        "- Split each line to separate **sentences** from **labels**.  \n",
        "- Store the parsed data in a **DataFrame** with two columns: `\"Sentence\"` and `\"Label\"`.\n",
        "\n",
        "---\n",
        "\n",
        "## Steps  \n",
        "\n",
        "### 1. Import Pandas  \n",
        "\n",
        "```python\n",
        "# Import Pandas for data manipulation\n",
        "import pandas as pd\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Read the Text File  \n",
        "\n",
        "```python\n",
        "# Open and read the text file\n",
        "with open('Sentences_75Agree_sample.txt', 'r', encoding='latin1') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Display a sample of the data\n",
        "lines[:5]\n",
        "```\n",
        "\n",
        "#### Sample Output:  \n",
        "![Sample Data](https://udemy-images.s3.amazonaws.com:443/redactor/raw/create_lab_editor/2023-11-03_10-05-40-ee8dfeb38fd4a35edce93efe1050031f.png)\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Parse Sentence-Label Pairs  \n",
        "\n",
        "```python\n",
        "# Split each line at '@' to separate sentences and labels\n",
        "data = [line.strip().split('@') for line in lines]\n",
        "```\n",
        "\n",
        "#### Parsed Data Sample:  \n",
        "![Parsed Data](https://udemy-images.s3.amazonaws.com:443/redactor/raw/create_lab_editor/2023-11-03_10-05-41-900a1f3f5f7ea0ee34e10981518d097c.png)\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Create a DataFrame  \n",
        "\n",
        "```python\n",
        "# Create a DataFrame with appropriate column names\n",
        "df = pd.DataFrame(data, columns=['Sentence', 'Label'])\n",
        "```\n",
        "---\n",
        "\n",
        "### 5. Preview the DataFrame  \n",
        "\n",
        "```python\n",
        "# Display the first few rows to verify structure\n",
        "df.head()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71c47d1c-5873-46a8-baf4-37d2e419300b",
      "metadata": {
        "id": "71c47d1c-5873-46a8-baf4-37d2e419300b"
      },
      "source": [
        "# Task 2: Analyze and Visualize Label and Sentence Length Distribution  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32ec75df-5330-404d-b76f-868047a42051",
      "metadata": {
        "id": "32ec75df-5330-404d-b76f-868047a42051"
      },
      "source": [
        "## Task Overview  \n",
        "In this task, you will analyze a **preloaded dataset** containing sentences and their corresponding labels. The goal is to:  \n",
        "- **Visualize the distribution of labels** to understand category frequencies in the dataset.  \n",
        "- **Calculate and plot the distribution of sentence lengths** to gain insights into sentence variability.\n",
        "\n",
        "---\n",
        "\n",
        "## Steps  \n",
        "\n",
        "### 1. Import Required Libraries  \n",
        "\n",
        "```python\n",
        "# Import Pandas for data handling and Matplotlib for visualization\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Convert Labels to a DataFrame  \n",
        "\n",
        "```python\n",
        "# Create a DataFrame for the 'Label' column to facilitate analysis\n",
        "labels_df = pd.DataFrame(df['Label'], columns=['Label'])\n",
        "\n",
        "# Display the first few rows of the label dataset\n",
        "labels_df.head()\n",
        "```\n",
        "\n",
        "#### Sample Output:  \n",
        "![Labels DataFrame](https://udemy-images.s3.amazonaws.com:443/redactor/raw/create_lab_editor/2023-11-03_10-17-09-8a653a3b05bb202f2c63a2c48c22de9d.png)\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Plot the Label Distribution  \n",
        "\n",
        "```python\n",
        "# Use a bar chart to visualize the frequency of each label\n",
        "labels_df['Label'].value_counts().plot(kind='bar')\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Distribution of Labels')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "#### Label Distribution Plot:  \n",
        "![Label Distribution](https://udemy-images.s3.amazonaws.com:443/redactor/raw/create_lab_editor/2023-11-03_10-17-10-783c221429112b55eedb052659f656bf.png)\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Calculate Sentence Lengths  \n",
        "\n",
        "```python\n",
        "# Compute the number of words in each sentence\n",
        "sentence_lengths = [len(sentence.split()) for sentence in df['Sentence']]\n",
        "\n",
        "# Display the first few calculated sentence lengths\n",
        "sentence_lengths[:5]\n",
        "```\n",
        "\n",
        "#### Sample Output:  \n",
        "![Sentence Length Sample](https://udemy-images.s3.amazonaws.com:443/redactor/raw/create_lab_editor/2023-11-03_10-17-10-65d323d4fb34b3f9309c1a9ce5d73677.png)\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Plot the Distribution of Sentence Lengths  \n",
        "\n",
        "```python\n",
        "# Plot a histogram to visualize sentence length distribution\n",
        "plt.hist(sentence_lengths, bins=30)\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Distribution of Sentence Lengths')\n",
        "plt.xlabel('Sentence Length')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "#### Sentence Length Distribution Plot:  \n",
        "![Sentence Length Distribution](https://udemy-images.s3.amazonaws.com:443/redactor/raw/create_lab_editor/2023-11-03_10-17-10-4430d907956357f00164035c08e148c8.png)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ade6f3cd-9bf7-4e44-b02b-c42ea375f6d6",
      "metadata": {
        "id": "ade6f3cd-9bf7-4e44-b02b-c42ea375f6d6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "1014641c-1c74-4561-8fb5-44f88f4fc7a3",
      "metadata": {
        "id": "1014641c-1c74-4561-8fb5-44f88f4fc7a3"
      },
      "source": [
        "# Task 3: Exploring Label Distribution and Sentence Lengths in Text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7e12919-5b1f-43fc-839d-861c5b8d02d9",
      "metadata": {
        "id": "f7e12919-5b1f-43fc-839d-861c5b8d02d9"
      },
      "source": [
        "### 1. Import Necessary Libraries  \n",
        "\n",
        "```python\n",
        "# Import Counter for frequency analysis and Seaborn for visualization\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Tokenize Sentences into Words  \n",
        "\n",
        "```python\n",
        "# Flatten the list of sentences into individual words\n",
        "words = [word for sentence in df['Sentence'] for word in sentence.split()]\n",
        "\n",
        "# Display the first 10 words\n",
        "words[:10]\n",
        "```\n",
        "\n",
        "#### Sample Output:  \n",
        "![Tokenized Words](https://udemy-images.s3.amazonaws.com:443/redactor/raw/create_lab_editor/2023-11-03_10-30-42-4a8c373d492bb7ac5e35494697f426df.png)\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Get the Most Common Words  \n",
        "\n",
        "```python\n",
        "# Use Counter to find the top 20 most common words\n",
        "common_words = Counter(words).most_common(20)\n",
        "\n",
        "# Display the first 5 most common words\n",
        "common_words[:5]\n",
        "```\n",
        "\n",
        "#### Sample Output:  \n",
        "![Most Common Words](https://udemy-images.s3.amazonaws.com:443/redactor/raw/create_lab_editor/2023-11-03_10-30-42-dad034e8bb7e3130b2440f6661abd589.png)\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Convert Common Words to a DataFrame  \n",
        "\n",
        "```python\n",
        "# Convert common words into a DataFrame for visualization\n",
        "common_words_df = pd.DataFrame(common_words, columns=['Word', 'Frequency'])\n",
        "\n",
        "# Display the top rows of the DataFrame\n",
        "common_words_df.head()\n",
        "```\n",
        "\n",
        "#### Sample Output:  \n",
        "![Common Words DataFrame](https://udemy-images.s3.amazonaws.com:443/redactor/raw/create_lab_editor/2023-11-03_10-30-42-37e9a8ab2deae2bc299124af00df3a05.png)\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Plot the Most Common Words with Customizations  \n",
        "\n",
        "```python\n",
        "# Set Seaborn theme for a clean look\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# Create a horizontal bar plot with a custom color and updated labels\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=common_words_df, x='Frequency', y='Word', palette=\"magma\")\n",
        "\n",
        "# Customize labels\n",
        "plt.title('Top 20 Most Frequent Words in Text Data', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Word Count', fontsize=12)\n",
        "plt.ylabel('Word', fontsize=12)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "#### Custom Word Frequency Plot:  \n",
        "![Customized Word Frequency Plot](./Most_Common_Words.PNG)\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52bd9bcc-b987-47b8-938d-3b85fa82d6b2",
      "metadata": {
        "id": "52bd9bcc-b987-47b8-938d-3b85fa82d6b2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "bb34be5b-ce9f-4cd2-bae1-a0b50ae29455",
      "metadata": {
        "id": "bb34be5b-ce9f-4cd2-bae1-a0b50ae29455"
      },
      "source": [
        "# Task 4: Bigram Frequency Analysis and Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e6be0a5-0894-4834-9945-618de991f6a8",
      "metadata": {
        "id": "8e6be0a5-0894-4834-9945-618de991f6a8"
      },
      "source": [
        "## Task Overview  \n",
        "In this task, you will process text data to identify and visualize the **20 most frequently occurring words** in the dataset. This involves:  \n",
        "- **Tokenizing sentences** into words.  \n",
        "- **Counting word occurrences** using `Counter`.  \n",
        "- **Creating a bar plot** to visualize the most common words and their frequencies.\n",
        "\n",
        "---\n",
        "\n",
        "## Steps  \n",
        "\n",
        "### 1. Import Necessary Libraries  \n",
        "\n",
        "```python\n",
        "# Import Counter for frequency analysis and Seaborn for visualization\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Tokenize Sentences into Words  \n",
        "\n",
        "```python\n",
        "# Flatten the list of sentences into individual words\n",
        "words = [word for sentence in df['Sentence'] for word in sentence.split()]\n",
        "\n",
        "# Display the first 10 words\n",
        "words[:10]\n",
        "```\n",
        "\n",
        "#### Sample Output:  \n",
        "![Tokenized Words](https://udemy-images.s3.amazonaws.com:443/redactor/raw/create_lab_editor/2023-11-03_10-30-42-4a8c373d492bb7ac5e35494697f426df.png)\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Get the Most Common Words  \n",
        "\n",
        "```python\n",
        "# Use Counter to find the top 20 most common words\n",
        "common_words = Counter(words).most_common(20)\n",
        "\n",
        "# Display the first 5 most common words\n",
        "common_words[:5]\n",
        "```\n",
        "\n",
        "#### Sample Output:  \n",
        "![Most Common Words](https://udemy-images.s3.amazonaws.com:443/redactor/raw/create_lab_editor/2023-11-03_10-30-42-dad034e8bb7e3130b2440f6661abd589.png)\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Convert Common Words to a DataFrame  \n",
        "\n",
        "```python\n",
        "# Convert common words into a DataFrame for visualization\n",
        "common_words_df = pd.DataFrame(common_words, columns=['Word', 'Frequency'])\n",
        "\n",
        "# Display the top rows of the DataFrame\n",
        "common_words_df.head()\n",
        "```\n",
        "\n",
        "#### Sample Output:  \n",
        "![Common Words DataFrame](https://udemy-images.s3.amazonaws.com:443/redactor/raw/create_lab_editor/2023-11-03_10-30-42-37e9a8ab2deae2bc299124af00df3a05.png)\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Plot the Most Common Words  \n",
        "\n",
        "```python\n",
        "# Create a horizontal bar plot using Seaborn\n",
        "sns.barplot(data=common_words_df, x='Frequency', y='Word')\n",
        "\n",
        "# Add title\n",
        "plt.title('Most Common Words')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "#### Word Frequency Plot:  \n",
        "![Most Common Words Plot](./Most_Common_BiGrams.PNG)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfa60779-7e8a-4f9a-8d9b-8a2b2621d7a0",
      "metadata": {
        "id": "bfa60779-7e8a-4f9a-8d9b-8a2b2621d7a0"
      },
      "source": [
        "# Task 5: Sentiment Distribution Analysis Using Word Frequencies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca034870-ed81-487f-915b-4fbd86703e85",
      "metadata": {
        "id": "ca034870-ed81-487f-915b-4fbd86703e85"
      },
      "source": [
        "## **Task Overview**  \n",
        "In this task, you will analyze text sentiment by counting the occurrences of predefined **positive** and **negative** words within a dataset of sentences. The goal is to estimate the **sentiment distribution** in the text data.\n",
        "\n",
        "### **Steps to Complete the Task:**\n",
        "1. **Define sentiment word lists**  \n",
        "   - Create two lists:  \n",
        "     - One containing **six** positive sentiment words (*e.g., good, great, profit*).  \n",
        "     - Another containing **six** negative sentiment words (*e.g., bad, poor, loss*).  \n",
        "\n",
        "2. **Count the occurrences** of these words in the dataset.  \n",
        "\n",
        "3. **Visualize the sentiment distribution** using a **bar plot**.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Define Sentiment Word Lists**  \n",
        "```python\n",
        "# Lists of predefined positive and negative sentiment words\n",
        "positive_words = ['good', 'great', 'positive', 'profit', 'up', 'increase']\n",
        "negative_words = ['bad', 'poor', 'negative', 'loss', 'down', 'decrease']\n",
        "```\n",
        "\n",
        "#### **Sample Output:**  \n",
        "![Sentiment Word Lists](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_10-56-08-413558016b116bd05a5029792ddbee5c.png)\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Count Occurrences of Sentiment Words**  \n",
        "```python\n",
        "# Count occurrences of positive and negative words in the dataset\n",
        "positive_counts = sum(sentence.lower().count(word) for sentence in df['Sentence'] for word in positive_words)\n",
        "negative_counts = sum(sentence.lower().count(word) for sentence in df['Sentence'] for word in negative_words)\n",
        "```\n",
        "\n",
        "#### **Sample Output:**  \n",
        "![Sentiment Word Counts](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_10-56-08-413558016b116bd05a5029792ddbee5c.png)\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Visualize Sentiment Word Counts**  \n",
        "```python\n",
        "# Import necessary libraries for visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a bar plot for sentiment word counts with custom colors\n",
        "sns.barplot(x=['Positive Words', 'Negative Words'], y=[positive_counts, negative_counts], palette=\"coolwarm\")\n",
        "\n",
        "# Customize the plot labels\n",
        "plt.title('Occurrence of Sentiment Words', fontsize=14, fontweight='bold')  \n",
        "plt.ylabel('Word Count', fontsize=12)  \n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "#### **Sentiment Word Count Visualization:**  \n",
        "![Sentiment Word Count Plot](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_10-56-08-413558016b116bd05a5029792ddbee5c.png)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1676e9ec-9122-4b58-b12d-a931b4af6a3d",
      "metadata": {
        "id": "1676e9ec-9122-4b58-b12d-a931b4af6a3d"
      },
      "source": [
        "# Task 6: Data Preprocessing for Sentiment Analysisl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61149cb8-7198-47dd-b721-b78aa7d70238",
      "metadata": {
        "id": "61149cb8-7198-47dd-b721-b78aa7d70238"
      },
      "source": [
        "## **Task Overview**  \n",
        "In this task, you will **preprocess and transform textual data** to make it suitable for **machine learning models**. This includes:\n",
        "1. **Cleaning the text data** to remove unwanted characters and standardize format.  \n",
        "2. **Converting the cleaned text** into numerical representations using **TF-IDF vectorization**.  \n",
        "3. **Encoding categorical labels** into numerical values.  \n",
        "4. **Splitting the data** into training and test sets (80%-20%).  \n",
        "5. **Ensuring data compatibility** for machine learning models.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Clean and Standardize Text Data**\n",
        "### **Define a Function to Clean Sentences**\n",
        "```python\n",
        "import re\n",
        "\n",
        "# Function to clean text data\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"[^\\w\\s]\", '', text)  # Remove punctuation\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r\"\\s+\", ' ', text)  # Collapse multiple spaces to one\n",
        "    return text.strip()  # Remove leading and trailing spaces\n",
        "```\n",
        "\n",
        "### **Apply Text Cleaning Function**\n",
        "```python\n",
        "# Clean sentences in the DataFrame\n",
        "df['Cleaned_Sentence'] = df['Sentence'].apply(clean_text)\n",
        "df.head()\n",
        "```\n",
        "\n",
        "#### **Sample Output:**  \n",
        "![Cleaned Sentences Output](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_13-04-35-c8e7ceabf1162440b749a69826aed476.png)\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Convert Text into TF-IDF Features**\n",
        "### **Apply TF-IDF Vectorization**\n",
        "```python\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize and apply TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['Cleaned_Sentence'])\n",
        "X\n",
        "```\n",
        "\n",
        "#### **Sample Output:**  \n",
        "![TF-IDF Vectorization Output](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_13-04-36-7f1ecfe628a81667369063a89119d739.png)\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Encode Text Labels as Numbers**\n",
        "### **Convert Labels to Numerical Format**\n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode text labels into integers\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df['Label'])\n",
        "y\n",
        "```\n",
        "\n",
        "#### **Sample Output:**  \n",
        "![Encoded Labels Output](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_13-04-36-ef1437180aa74f96e059c93c19a80e1f.png)\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Split Data into Training and Test Sets**\n",
        "### **Create an 80-20 Split**\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=1502)\n",
        "\n",
        "X_train\n",
        "```\n",
        "\n",
        "#### **Sample Output:**  \n",
        "![Train-Test Split Output](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_13-04-36-183c4f4d363f908d5c4ec07b971be538.png)\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Convert Sparse Matrix to Dense Format**\n",
        "### **Ensure Compatibility with ML Models**\n",
        "```python\n",
        "# Convert sparse matrices to dense for model compatibility\n",
        "X_train_dense = X_train.todense()\n",
        "X_test_dense = X_test.todense()\n",
        "X_train_dense\n",
        "```\n",
        "\n",
        "#### **Sample Output:**  \n",
        "![Dense Matrix Output](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_13-04-36-7662a81b05563a813efc88b8fffdd419.png)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c2724d8-8a9c-4a48-a8c0-b0b60d658c0c",
      "metadata": {
        "id": "2c2724d8-8a9c-4a48-a8c0-b0b60d658c0c"
      },
      "source": [
        "# Task 7: Build and Evaluate a Multi-Class Classification Neural Network with TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82285e7c-2407-4819-b108-411cbc8b4a5e",
      "metadata": {
        "id": "82285e7c-2407-4819-b108-411cbc8b4a5e"
      },
      "source": [
        "## **Task Overview**\n",
        "In this task, you will build a **neural network model** using **TensorFlow's Sequential API** to perform **multi-class classification** on a dataset with three sentiment classes: **Positive, Neutral, and Negative**.\n",
        "\n",
        "You will:\n",
        "1. **Construct a Sequential model** with specified layers.\n",
        "2. **Compile the model** using appropriate loss, optimizer, and metrics.\n",
        "3. **Train the model** for 10 epochs and validate performance.\n",
        "4. **Evaluate model accuracy** on test data.\n",
        "5. **Visualize training progress** with accuracy and loss graphs.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Define the Neural Network Model**\n",
        "### **Initialize the Sequential Model**\n",
        "```python\n",
        "# Import TensorFlow\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the Sequential model with specified layers\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer with 64 neurons\n",
        "    tf.keras.layers.Dropout(0.5),  # Dropout layer with 50% drop rate for regularization\n",
        "    tf.keras.layers.Dense(3, activation='softmax')  # Output layer with 3 classes (softmax for multi-class classification)\n",
        "])\n",
        "```\n",
        "\n",
        "#### **Model Initialization Output:**  \n",
        "![Model Initialization](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_13-23-54-ae23a2abbd08c2f25ccc5dbd111f2762.png)\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Compile the Model**\n",
        "### **Set Up Optimizer, Loss Function, and Metrics**\n",
        "```python\n",
        "# Compile the model with optimizer, loss function, and evaluation metric\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Display Model Summary**\n",
        "### **Inspect the Model Architecture**\n",
        "```python\n",
        "# Display model summary to understand layer structure\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "#### **Model Summary Output:**  \n",
        "![Model Summary](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_13-23-54-4d21fe0b5253884e81f449059483eb2e.png)\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Train the Model**\n",
        "### **Fit the Model on Training Data**\n",
        "```python\n",
        "# Train the model with training data and validate on test data\n",
        "history = model.fit(X_train_dense, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(X_test_dense, y_test))\n",
        "```\n",
        "\n",
        "#### **Training Output:**  \n",
        "![Model Training](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_13-23-54-a2bf97d36436a7fcd34643a18afa623a.png)\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Evaluate the Model Performance**\n",
        "### **Test Accuracy and Loss**\n",
        "```python\n",
        "# Evaluate model on test dataset\n",
        "loss, accuracy = model.evaluate(X_test_dense, y_test)\n",
        "\n",
        "# Print loss and accuracy\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')\n",
        "```\n",
        "\n",
        "#### **Evaluation Output:**  \n",
        "![Model Evaluation](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_13-23-55-be2f54d42481851102f9017bdd0a47ce.png)\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Visualize Training Progress**\n",
        "### **Plot Training vs Validation Accuracy and Loss**\n",
        "```python\n",
        "# Import matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set up figure size\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# Adjust layout and show plots\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "#### **Accuracy & Loss Visualization Output:**  \n",
        "![Model Performance Plots](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_13-23-55-b5e9a634122d9868d290bce24e7fd728.png)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "809e26eb-9f5e-43ce-af4c-fa860bfb6a1b",
      "metadata": {
        "id": "809e26eb-9f5e-43ce-af4c-fa860bfb6a1b"
      },
      "source": [
        "# Task 8: Hyperparameter Tuning with Keras Tuner for Multi-Class Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed0e5887-81d0-4fd3-a976-e17c1626e15d",
      "metadata": {
        "id": "ed0e5887-81d0-4fd3-a976-e17c1626e15d"
      },
      "source": [
        "### **Task Overview**\n",
        "In this task, you will **optimize a neural network** for **multi-class classification** using **Keras Tuner**. The tuner will explore different combinations of:\n",
        "- **Neurons in the dense layer** (from 64 to 512, step size of 64).\n",
        "- **Activation functions** ('relu' or 'tanh').\n",
        "- **Dropout rates** (ranging from 0.0 to 0.5, step size of 0.1).\n",
        "\n",
        "The tuner will **run 20 trials**, with each trial executed **3 times** to account for performance variability.\n",
        "\n",
        "---\n",
        "\n",
        "## **1️. Import Required Libraries**\n",
        "```python\n",
        "# Import Keras Tuner\n",
        "import keras_tuner as kt\n",
        "```\n",
        "#### ** Expected Output:**  \n",
        "![Import Keras Tuner](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_14-08-51-4c7e06a5cb034b848359ace46f4d424f.png)\n",
        "\n",
        "---\n",
        "\n",
        "## **2️. Define the Hypermodel Function**\n",
        "```python\n",
        "# Function to build the model given hyperparameters\n",
        "def build_model(hp):\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Dense layer with tunable number of units and activation function\n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        units=hp.Int('units', min_value=64, max_value=512, step=64),\n",
        "        activation=hp.Choice('activation', values=['relu', 'tanh']),\n",
        "        input_shape=(X_train.shape[1],)\n",
        "    ))\n",
        "\n",
        "    # Dropout layer with tunable rate\n",
        "    model.add(tf.keras.layers.Dropout(\n",
        "        rate=hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)\n",
        "    ))\n",
        "\n",
        "    # Output layer for 3-class classification\n",
        "    model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "```\n",
        "\n",
        "#### **Expected Output:**  \n",
        "![Hypermodel Definition](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_14-08-51-b61c90603cc92341327946e86f402d8d.png)\n",
        "\n",
        "---\n",
        "\n",
        "## **3️. Configure and Run Hyperparameter Search**\n",
        "```python\n",
        "# Initialize the random search tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=20,\n",
        "    executions_per_trial=3,\n",
        "    directory='tuner_directory',\n",
        "    project_name='hyperparameter_tuning'\n",
        ")\n",
        "\n",
        "# Execute the search over the specified epochs and validation data\n",
        "tuner.search(\n",
        "    X_train_dense, y_train,\n",
        "    epochs=10,\n",
        "    validation_data=(X_test_dense, y_test)\n",
        ")\n",
        "```\n",
        "#### **Expected Output:**  \n",
        "![Hyperparameter Search](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_14-08-51-535d318ea68f99b60bd718b388bd8484.png)\n",
        "\n",
        "---\n",
        "\n",
        "## **4️. Review Search Results**\n",
        "```python\n",
        "# Output the summary of the hyperparameter tuning results\n",
        "tuner.results_summary()\n",
        "```\n",
        "#### **Expected Output:**  \n",
        "![Tuning Results Summary](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_14-08-51-535d318ea68f99b60bd718b388bd8484.png)\n",
        "\n",
        "---\n",
        "\n",
        "## **5️. Extract the Best Hyperparameters**\n",
        "```python\n",
        "# Get the best set of hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"\"\"\n",
        "Best Number of Units: {best_hps.get('units')}\n",
        "Best Activation Function: {best_hps.get('activation')}\n",
        "Best Dropout Rate: {best_hps.get('dropout_rate')}\n",
        "\"\"\")\n",
        "```\n",
        "#### **Expected Output:**  \n",
        "![Best Hyperparameters](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_14-08-52-9a9924f0e1beacab5e6cd1c353f5bbe0.png)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c41881fb-b41c-4886-a684-7548d3e64f4a",
      "metadata": {
        "id": "c41881fb-b41c-4886-a684-7548d3e64f4a"
      },
      "source": [
        "# Task 9: Advanced Hyperparameter Tuning for NLP Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96c7e244-fefd-4c51-ae07-855ab1ea20d0",
      "metadata": {
        "id": "96c7e244-fefd-4c51-ae07-855ab1ea20d0"
      },
      "source": [
        "### **Task Overview**\n",
        "Building upon the **first round of hyperparameter tuning**, you will now **refine the model further** by:\n",
        "- **Adding a second dense layer** with a tunable number of units.\n",
        "- **Introducing optimizer selection** between **Adam** and **SGD**.\n",
        "- **Tuning the learning rate** with values **(1e-2, 1e-3, 1e-4)**.\n",
        "- **Applying conditional logic** to select the best optimizer based on the hyperparameter search.\n",
        "\n",
        "---\n",
        "\n",
        "## **1️. Define the Advanced Hyperparameter Model**\n",
        "```python\n",
        "def build_model_step2(hp):\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Add the first dense layer using the best parameters from step 1\n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        units=tuner_step1.get_best_hyperparameters()[0].get('units'),\n",
        "        activation=tuner_step1.get_best_hyperparameters()[0].get('activation'),\n",
        "        input_shape=(X_train.shape[1],)\n",
        "    ))\n",
        "\n",
        "    # Apply dropout using the best rate from step 1\n",
        "    model.add(tf.keras.layers.Dropout(\n",
        "        rate=tuner_step1.get_best_hyperparameters()[0].get('dropout_rate')\n",
        "    ))\n",
        "\n",
        "    # Add a second dense layer where units are a new tunable hyperparameter\n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        units=hp.Int('second_units', min_value=32, max_value=512, step=32),\n",
        "        activation='relu'\n",
        "    ))\n",
        "\n",
        "    # Select optimizer and learning rate as new hyperparameters\n",
        "    optimizer_selected = hp.Choice('optimizer', values=['adam', 'sgd'])\n",
        "    learning_rate_selected = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    # Apply conditional logic for optimizer selection\n",
        "    if optimizer_selected == 'adam':\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_selected)\n",
        "    else:\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate_selected)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "```\n",
        "#### **Expected Output:**  \n",
        "![Refined Model Setup](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_15-16-42-69df901f3d609f83b6b7d31be98d178e.png)\n",
        "\n",
        "---\n",
        "\n",
        "## **2️. Initialize and Run the Second Hyperparameter Search**\n",
        "```python\n",
        "# Initialize the second tuner with an expanded search space\n",
        "tuner_step2 = kt.RandomSearch(\n",
        "    build_model_step2,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=20,\n",
        "    executions_per_trial=3,\n",
        "    directory='tuner_step2_directory',\n",
        "    project_name='step2_tuning'\n",
        ")\n",
        "\n",
        "# Execute the search\n",
        "tuner_step2.search(\n",
        "    X_train_dense, y_train,\n",
        "    epochs=10,\n",
        "    validation_data=(X_test_dense, y_test)\n",
        ")\n",
        "```\n",
        "#### **Expected Output:**  \n",
        "![Hyperparameter Search](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_15-16-42-4eeabe8f6964b24bfcdc0c2b05be5203.png)\n",
        "\n",
        "---\n",
        "\n",
        "## **3️. Review Search Results**\n",
        "```python\n",
        "# Output the summary of the hyperparameter tuning results\n",
        "tuner_step2.results_summary()\n",
        "```\n",
        "#### **Expected Output:**  \n",
        "![Tuning Results Summary](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_15-16-42-4eeabe8f6964b24bfcdc0c2b05be5203.png)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "587b5947-10b6-4f0b-9e9f-563db124b8d1",
      "metadata": {
        "id": "587b5947-10b6-4f0b-9e9f-563db124b8d1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e6e5946b-2e55-4d27-8f42-9b77d42c2197",
      "metadata": {
        "id": "e6e5946b-2e55-4d27-8f42-9b77d42c2197"
      },
      "source": [
        "# Task 10:Final Model Implementation and Saving"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c579fde1-54a2-46b3-be85-7e5f6319a876",
      "metadata": {
        "id": "c579fde1-54a2-46b3-be85-7e5f6319a876"
      },
      "source": [
        "### **Task Overview**  \n",
        "After performing **two rounds of hyperparameter tuning**, we will now:  \n",
        "✔ **Build the final model** using the best hyperparameters.  \n",
        "✔ **Train the model** with the full dataset.  \n",
        "✔ **Evaluate its performance** on the test data.  \n",
        "✔ **Save the final model** for future use.  \n",
        "\n",
        "---\n",
        "\n",
        "### **1. Define and Build the Final Model**\n",
        "```python\n",
        "def build_final_model(best_hps_step1, best_hps_step2):\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # First Dense Layer with Best Hyperparameters from Step 1\n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        units=best_hps_step1.get('units'),\n",
        "        activation=best_hps_step1.get('activation'),\n",
        "        input_shape=(X_train.shape[1],)\n",
        "    ))\n",
        "\n",
        "    # Dropout Layer with Best Rate from Step 1\n",
        "    model.add(tf.keras.layers.Dropout(best_hps_step1.get('dropout_rate')))\n",
        "\n",
        "    # Second Dense Layer with Best Units from Step 2\n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        units=best_hps_step2.get('second_units'),\n",
        "        activation='relu'\n",
        "    ))\n",
        "\n",
        "    # Select Optimizer and Learning Rate from Step 2\n",
        "    optimizer_selected = best_hps_step2.get('optimizer')\n",
        "    learning_rate_selected = best_hps_step2.get('learning_rate')\n",
        "\n",
        "    # Conditional Optimizer Selection\n",
        "    if optimizer_selected == 'adam':\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_selected)\n",
        "    else:\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate_selected)\n",
        "\n",
        "    # Compile the Model\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Retrieve Best Hyperparameters from Tuners\n",
        "best_hps_step1 = tuner_step1.get_best_hyperparameters()[0]\n",
        "best_hps_step2 = tuner_step2.get_best_hyperparameters()[0]\n",
        "\n",
        "# Build the Final Model\n",
        "final_model = build_final_model(best_hps_step1, best_hps_step2)\n",
        "```\n",
        "#### **Expected Output:**  \n",
        "**Final Model Ready with Best Hyperparameters!**  \n",
        "![Final Model Setup](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_15-20-09-20282cedcb07d68d194d590df6254b1b.png)\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Train the Final Model**\n",
        "```python\n",
        "# Train the Final Model\n",
        "history_final = final_model.fit(\n",
        "    X_train_dense, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_dense, y_test)\n",
        ")\n",
        "```\n",
        "#### **Expected Output:**  \n",
        "**Training in Progress...**  \n",
        "![Training Process](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_15-20-09-20282cedcb07d68d194d590df6254b1b.png)\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Evaluate Model Performance**\n",
        "```python\n",
        "# Evaluate the Final Model\n",
        "final_loss, final_accuracy = final_model.evaluate(X_test_dense, y_test)\n",
        "\n",
        "# Print Evaluation Results\n",
        "print(f'Final Model Loss: {final_loss}')\n",
        "print(f'Final Model Accuracy: {final_accuracy}')\n",
        "```\n",
        "#### **Expected Output:**  \n",
        "**Final Evaluation Metrics Ready!**  \n",
        "![Model Evaluation](https://udemy-images.s3.amazonaws.com/redactor/raw/create_lab_editor/2023-11-03_15-20-09-4b43578c6228d08325e73197cd954c05.png)\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Save the Final Model for Future Use**\n",
        "```python\n",
        "# Save the Final Trained Model\n",
        "final_model.save('finalModel.h5')\n",
        "\n",
        "print(\"Final model saved to 'finalModel.h5'\")\n",
        "```\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
